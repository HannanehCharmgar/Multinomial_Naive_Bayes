

#  الگوریتم Multinomial Naive Bayes (فیلتر اسپم)

این آموزش بر نحوه استفاده از طبقه‌بندی‌کننده Naive Bayes برای تفکیک پیام‌های عادی (Normal) از پیام‌های اسپم (Spam) تمرکز دارد.

# **بخش ۱: گام‌های اولیه و محاسبه احتمال کلمات**

هدف اولیه این است که بدانیم هر کلمه در یک دسته خاص (عادی یا اسپم) چقدر احتمال حضور دارد. این احتمالات، **Likelihoods** نامیده می‌شوند.

### **۱. ساخت هیستوگرام کلمات**

ما دو هیستوگرام (شمارش فراوانی) می‌سازیم: یکی برای تمام کلمات پیام‌های عادی و دیگری برای کلمات پیام‌های اسپم.
## پیام های عادی:
<img width="445" height="235" alt="image" src="https://github.com/user-attachments/assets/8829e4a3-624c-42c9-8b0f-d529e9fa4ce2" />

در این تصویر، فراوانی کلمات رایج در پیام‌های عادی نمایش داده شده است.  
الگوریتم Multinomial Naive Bayes از تعداد تکرار این کلمات برای یادگیری الگوی پیام‌های غیر اسپم استفاده می‌کند.

## پیام های اسپم:

<img width="441" height="204" alt="image" src="https://github.com/user-attachments/assets/5e0e477b-238f-4b9f-8817-55c2958541a9" />

در این تصویر، کلماتی که بیشتر در پیام‌های اسپم ظاهر می‌شوند نشان داده شده‌اند.  
الگوریتم با مقایسه توزیع فراوانی کلمات، احتمال اسپم بودن یک پیام جدید را محاسبه و آن را دسته‌بندی می‌کند.

---

### **۲. محاسبه Likelihoods (احتمالات شرطی)**

احتمالات شرطی یا Likelihood نشان‌دهنده احتمال دیدن یک کلمه *به شرطی که* در یک پیام از یک کلاس خاص (مثلاً عادی) باشد.

*   **مثال (برای کلمه "dear" در پیام عادی):**
    احتمال دیدن کلمه "dear" در یک پیام عادی برابر است با:
    $$\frac{\text{تعداد تکرار "dear" در پیام‌های عادی}}{\text{تعداد کل کلمات موجود در پیام‌های عادی}}$$
    اگر "dear" هشت بار و مجموع کلمات عادی ۱۷ کلمه باشد: $8 / 17 = 0.47$.

*   **مثال (برای کلمه "dear" در پیام اسپم):**
    اگر "dear" دو بار و مجموع کلمات اسپم ۷ کلمه باشد: $2 / 7 = 0.29$.



# **بخش ۲: فرآیند طبقه‌بندی پیام جدید**

فرض کنید می‌خواهیم پیام **"dear friend"** را طبقه‌بندی کنیم.

### **1. حدس اولیه (Prior Probability)**

قبل از بررسی محتوای پیام، ما یک حدس اولیه در مورد احتمال کلی عادی یا اسپم بودن آن می‌زنیم که به آن **Prior Probability** گفته می‌شود.

*   **روش تخمین:** از داده‌های آموزشی استفاده می‌شود. اگر ۸ پیام از ۱۲ پیام، عادی باشند، حدس اولیه ما برای عادی بودن: $8 / 12 = 0.67$.
*   اگر ۴ پیام از ۱۲ پیام، اسپم باشند، حدس اولیه ما برای اسپم بودن: $4 / 12 = 0.33$.


---

### 2. قانون بیز (Bayes’ Theorem)

قانون بیز می‌گوید احتمال اینکه یک پیام متعلق به کلاس \(C\) باشد با داشتن ویژگی‌ها \(X = (x_1, x_2, ..., x_n)\) برابر است با:

$\[
P(C | X) = \frac{P(C) \cdot P(X | C)}{P(X)}
\]$

- \(P(C)\) → **احتمال پیشین (Prior)** کلاس  
- \(P(X | C)\) → **احتمال شرطی (Likelihood)** ویژگی‌ها با توجه به کلاس  
- \(P(C | X)\) → **احتمال پسین (Posterior)** که هدف مدل است

---

### 3. فرضیات Naive Bayes

- فرض می‌کنیم ویژگی‌ها (کلمات) **مستقل از هم هستند**.  
- بنابراین احتمال کل ویژگی‌ها با توجه به کلاس، حاصل ضرب احتمال هر ویژگی است:

$\[
P(X | C) = P(x_1 | C) \times P(x_2 | C) \times ... \times P(x_n | C)
\]$

---

### 4.محاسبه امتیاز (Score) برای هر کلاس

#### کلاس Normal (عادی):

$\[
Score_{Normal} \propto P(Normal) \times P(dear|Normal) \times P(friend|Normal)
\]$

با جایگذاری مقادیر فرضی:

$\[
Score_{Normal} = 0.67 \times 0.47 \times 0.29 \approx 0.09
\]$

#### کلاس Spam :

$\[
Score_{Spam} \propto P(Spam) \times P(dear|Spam) \times P(friend|Spam)
\]$

با جایگذاری مقادیر:

$\[
Score_{Spam} \approx 0.01
\]$

---

### ۴️⃣ تصمیم‌گیری نهایی

- کلاسی که **بیشترین امتیاز (Score)** را دارد، انتخاب می‌شود:

$\[
\text{Predicted Class} = \arg\max_C Score_C
\]$

در مثال ما:

- Score_Normal ≈ 0.09  
- Score_Spam ≈ 0.01  

→ پیام **عادی (Normal)** پیش‌بینی می‌شود.

---

### ✅ جمع‌بندی

1. ابتدا **Prior** کلاس‌ها محاسبه می‌شود.  
2. سپس **Likelihood** هر کلمه با توجه به کلاس محاسبه می‌شود.  
3. با ضرب Likelihood ها در Prior، **Score** برای هر کلاس بدست می‌آید.  
4. در نهایت، **کلاس با بیشترین Score** انتخاب می‌شود.

### **۳. تصمیم‌گیری**

پیام به کلاسی اختصاص داده می‌شود که بالاترین امتیاز را کسب کند.
در مثال "dear friend"، از آنجا که $0.09 > 0.01$، پیام به عنوان **عادی (Normal)** طبقه‌بندی می‌شود.

## **بخش ۳: مشکل صفر شدن و راهکار (Smoothing)**

### **۱. مشکل صفر (Zero Frequency)**

اگر یک پیام جدید (مانند "lunch money money money money") حاوی کلمه‌ای باشد که در مجموعه داده آموزشی اسپم دیده نشده است (مثلاً کلمه "lunch")، احتمال آن کلمه در اسپم صفر خواهد بود: $P(\text{lunch} | \text{Spam}) = 0$.

*   **نتیجه:** هر چیزی در صفر ضرب شود، صفر می‌شود. بنابراین، اگر کلمه "lunch" در پیام باشد، امتیاز نهایی اسپم **همیشه صفر** خواهد بود، حتی اگر پیام حاوی کلمات رایج اسپم باشد.

### **۲. راهکار: افزودن آلفا (Alpha Smoothing)**

برای اطمینان از اینکه هیچ احتمالی صفر نمی‌شود، یک واحد شمارش (به نام $\alpha$ که معمولاً $\alpha=1$ است) به هر کلمه در هیستوگرام‌ها اضافه می‌شود.

*   **تغییر در محاسبه Likelihood:**
    به عنوان مثال، برای محاسبه احتمال "lunch" در اسپم (که قبلاً ۰ بار دیده شده بود):
    $$\text{احتمال جدید } P(\text{lunch} | \text{Spam}) = \frac{\text{شمارش اصلی} + 1}{\text{تعداد کل کلمات قبلی} + \text{تعداد شمارش‌های اضافی}}$$
    این کار تضمین می‌کند که احتمال هرگز صفر نشود، مثلاً $1 / 11 \approx 0.09$.
*   **نکته:** افزودن این شمارش‌ها، حدس‌های اولیه (Prior) ما را تغییر نمی‌دهد، زیرا تعداد کل پیام‌های عادی یا اسپم در مجموعه آموزشی ثابت است.
*   **نتیجه:** با اعمال Smoothing، پیام "lunch money money money money" دیگر صفر نمی‌شود و امتیاز اسپم بالاتر از امتیاز عادی خواهد شد و به درستی به عنوان **اسپم** طبقه‌بندی می‌شود.

## **بخش ۴: چرا Naive Bayes، "ساده‌لوحانه" است؟**

این الگوریتم "ساده‌لوحانه" (Naive) نامیده می‌شود زیرا:

1.  **نادیده گرفتن ترتیب کلمات:** امتیاز کلمات "dear friend" و "friend dear" دقیقاً یکسان محاسبه می‌شود. ترتیب کلمات نادیده گرفته می‌شود.
2.  **مدل "کیسه کلمات" (Bag of Words):** Naive Bayes با زبان طوری برخورد می‌کند که انگار فقط یک "کیسه پر از کلمات" است و قواعد گرامری و عبارات رایج را نادیده می‌گیرد.
3.  **کارایی:** با وجود این ساده‌سازی (که در اصطلاح یادگیری ماشین، **High Bias** نامیده می‌شود)، Naive Bayes در عمل برای وظایفی مانند فیلتر کردن اسپم بسیار خوب عمل می‌کند (**Low Variance**).

---
## کاربردهای الگوریتم Multinomial Naive Bayes
- دسته‌بندی متن و اخبار
- تشخیص ایمیل‌های اسپم
- تحلیل احساسات نظرات کاربران
- تشخیص زبان متن
- برچسب‌گذاری خودکار اسناد
- دسته‌بندی تیکت‌ها و درخواست‌های پشتیبانی

## مزایای الگوریتم Multinomial Naive Bayes
- سرعت بسیار بالا در آموزش و پیش‌بینی
- عملکرد مناسب روی داده‌های متنی و ویژگی‌های TF-IDF
- پیاده‌سازی ساده و قابل فهم
- نیاز نداشتن به حجم زیاد داده آموزشی
- مقاوم در برابر نویز و ویژگی‌های کم‌اهمیت
- مناسب برای داده‌های با ابعاد بالا و پراکنده

## معایب الگوریتم Multinomial Naive Bayes
- فرض استقلال ویژگی‌ها که در عمل همیشه صحیح نیست
- ناتوانی در درک ترتیب و ارتباط معنایی کلمات
- محدود به داده‌های گسسته و شمارشی
- دقت کمتر نسبت به مدل‌های پیشرفته‌تر
- عملکرد ضعیف در متون وابسته به زمینه
