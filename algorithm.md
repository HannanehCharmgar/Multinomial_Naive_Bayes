

#  الگوریتم Multinomial Naive Bayes (فیلتر اسپم)

این آموزش بر نحوه استفاده از طبقه‌بندی‌کننده Naive Bayes برای تفکیک پیام‌های عادی (Normal) از پیام‌های اسپم (Spam) تمرکز دارد.

## **بخش ۱: گام‌های اولیه و محاسبه احتمال کلمات**

هدف اولیه این است که بدانیم هر کلمه در یک دسته خاص (عادی یا اسپم) چقدر احتمال حضور دارد. این احتمالات، **Likelihoods** نامیده می‌شوند.

### **۱. ساخت هیستوگرام کلمات**

ما دو هیستوگرام (شمارش فراوانی) می‌سازیم: یکی برای تمام کلمات پیام‌های عادی و دیگری برای کلمات پیام‌های اسپم.
## پیام های عادی:
<img width="445" height="235" alt="image" src="https://github.com/user-attachments/assets/8829e4a3-624c-42c9-8b0f-d529e9fa4ce2" />

## پیام های اسپم:

<img width="441" height="204" alt="image" src="https://github.com/user-attachments/assets/5e0e477b-238f-4b9f-8817-55c2958541a9" />


### **۲. محاسبه Likelihoods (احتمالات شرطی)**

Likelihood نشان‌دهنده احتمال دیدن یک کلمه *به شرطی که* در یک پیام از یک کلاس خاص (مثلاً عادی) باشد.

*   **مثال (برای کلمه "dear" در پیام عادی):**
    احتمال دیدن کلمه "dear" در یک پیام عادی برابر است با:
    $$\frac{\text{تعداد تکرار "dear" در پیام‌های عادی}}{\text{تعداد کل کلمات موجود در پیام‌های عادی}}$$
    اگر "dear" هشت بار و مجموع کلمات عادی ۱۷ کلمه باشد: $8 / 17 = 0.47$.

*   **مثال (برای کلمه "dear" در پیام اسپم):**
    اگر "dear" دو بار و مجموع کلمات اسپم ۷ کلمه باشد: $2 / 7 = 0.29$.

> **نکته اصطلاحی:** از آنجا که این احتمالات مربوط به کلمات گسسته هستند، گاهی به جای "احتمال" از اصطلاح "Likelihood" استفاده می‌شود. این دو اصطلاح در اینجا قابل جایگزینی هستند.

## **بخش ۲: فرآیند طبقه‌بندی پیام جدید**

فرض کنید می‌خواهیم پیام **"dear friend"** را طبقه‌بندی کنیم.

### **۱. حدس اولیه (Prior Probability)**

قبل از بررسی محتوای پیام، ما یک حدس اولیه در مورد احتمال کلی عادی یا اسپم بودن آن می‌زنیم که به آن **Prior Probability** گفته می‌شود.

*   **روش تخمین:** از داده‌های آموزشی استفاده می‌شود. اگر ۸ پیام از ۱۲ پیام، عادی باشند، حدس اولیه ما برای عادی بودن: $8 / 12 = 0.67$.
*   اگر ۴ پیام از ۱۲ پیام، اسپم باشند، حدس اولیه ما برای اسپم بودن: $4 / 12 = 0.33$.

### **۲. محاسبه امتیاز نهایی (Score)**

برای تعیین کلاس پیام، امتیازی متناسب با احتمال عضویت در هر کلاس محاسبه می‌کنیم.

*   **امتیاز برای پیام عادی (Normal):**

     $$$\text{Score}_{\text{Normal}} \propto \text{Prior}_{\text{Normal}} \times P(\text{dear} | \text{Normal}) \times P(\text{friend} | \text{Normal})$$$
    *   با جایگذاری مقادیر: $0.67 \times 0.47 \times 0.29 \approx 0.09$.

*   **امتیاز برای پیام اسپم (Spam):**

     $$$\text{Score}_{\text{Spam}} \propto \text{Prior}_{\text{Spam}} \times P(\text{dear} | \text{Spam}) \times P(\text{friend} | \text{Spam})$$$
    *   با جایگذاری مقادیر (فرضاً): $0.33 \times (\text{مقادیر Likelihood اسپم}) \approx 0.01$.

### **۳. تصمیم‌گیری**

پیام به کلاسی اختصاص داده می‌شود که بالاترین امتیاز را کسب کند.
در مثال "dear friend"، از آنجا که $0.09 > 0.01$، پیام به عنوان **عادی (Normal)** طبقه‌بندی می‌شود.

## **بخش ۳: مشکل صفر شدن و راهکار (Smoothing)**

### **۱. مشکل صفر (Zero Frequency)**

اگر یک پیام جدید (مانند "lunch money money money money") حاوی کلمه‌ای باشد که در مجموعه داده آموزشی اسپم دیده نشده است (مثلاً کلمه "lunch")، احتمال آن کلمه در اسپم صفر خواهد بود: $P(\text{lunch} | \text{Spam}) = 0$.

*   **نتیجه:** هر چیزی در صفر ضرب شود، صفر می‌شود. بنابراین، اگر کلمه "lunch" در پیام باشد، امتیاز نهایی اسپم **همیشه صفر** خواهد بود، حتی اگر پیام حاوی کلمات رایج اسپم باشد.

### **۲. راهکار: افزودن آلفا (Alpha Smoothing)**

برای اطمینان از اینکه هیچ احتمالی صفر نمی‌شود، یک واحد شمارش (به نام $\alpha$ که معمولاً $\alpha=1$ است) به هر کلمه در هیستوگرام‌ها اضافه می‌شود.

*   **تغییر در محاسبه Likelihood:**
    به عنوان مثال، برای محاسبه احتمال "lunch" در اسپم (که قبلاً ۰ بار دیده شده بود):
    $$\text{احتمال جدید } P(\text{lunch} | \text{Spam}) = \frac{\text{شمارش اصلی} + 1}{\text{تعداد کل کلمات قبلی} + \text{تعداد شمارش‌های اضافی}}$$
    این کار تضمین می‌کند که احتمال هرگز صفر نشود، مثلاً $1 / 11 \approx 0.09$.
*   **نکته:** افزودن این شمارش‌ها، حدس‌های اولیه (Prior) ما را تغییر نمی‌دهد، زیرا تعداد کل پیام‌های عادی یا اسپم در مجموعه آموزشی ثابت است.
*   **نتیجه:** با اعمال Smoothing، پیام "lunch money money money money" دیگر صفر نمی‌شود و امتیاز اسپم بالاتر از امتیاز عادی خواهد شد و به درستی به عنوان **اسپم** طبقه‌بندی می‌شود.

## **بخش ۴: چرا Naive Bayes، "ساده‌لوحانه" است؟**

این الگوریتم "ساده‌لوحانه" (Naive) نامیده می‌شود زیرا:

1.  **نادیده گرفتن ترتیب کلمات:** امتیاز کلمات "dear friend" و "friend dear" دقیقاً یکسان محاسبه می‌شود. ترتیب کلمات نادیده گرفته می‌شود.
2.  **مدل "کیسه کلمات" (Bag of Words):** Naive Bayes با زبان طوری برخورد می‌کند که انگار فقط یک "کیسه پر از کلمات" است و قواعد گرامری و عبارات رایج را نادیده می‌گیرد.
3.  **کارایی:** با وجود این ساده‌سازی (که در اصطلاح یادگیری ماشین، **High Bias** نامیده می‌شود)، Naive Bayes در عمل برای وظایفی مانند فیلتر کردن اسپم بسیار خوب عمل می‌کند (**Low Variance**).

---
